# Data 

## Sources
Weâ€™d like to find integrated data sets from the official website for research. Such data is more comprehensive and accurate, and it is also conducive to our data processing and analysis. Thus we choose NYC OpenData as our data sources. We tried to find data related to violence in New York City, so we decided to use complaint data of crime in NYC.

The dataset contains valid felony, misdemeanor, and violation crimes reported to the NYPD from 2006 to the end of 2019. Each record represents a criminal complaint in NYC. There are 35 columns including features like the type of crime, the location and time of enforcement, and the suspect and victim information. If we have any questions about the data, we can send a note to NYC OpenData (NYC Open Data - Contact Us (https://opendata.cityofnewyork.us/engage/)). 

There are several known issues relating to data quality. First, the dataset contains some null values, if the data was not collected or not available at the time of report. This may lead to bias in our study of the distribution of the data. Second, the dataset is released based on the date the incident was reported, but some crimes may have occurred years before they were reported. Also, some crimes that were not able to be geo-coded at the crime spot have been recorded as occurring at the police station house within the precinct of occurrence. This will cause a certain bias in the geographical analysis, but this bias can be avoided by appropriately expanding the geographical scope when grouping.

## Cleaning / transformation

We directly downloaded the data from the website and saved in a CSV file. When downloading, we filtered out the date range we are interested in - from 2017/01/01 to 2020/01/01. Most of the features in the dataset are in text format, and numeric values are in correct scales and format, so the data transformation is not necessary. There are several features where most of the information is not recorded, like name of NYC park, playground or greenspace of occurrence, name of NYCHA housing development of occurrence, and transit district in which the offense occurred, so we would not include them in our analysis. Also, there are pairs of features showing the same information, such as offense classification code & code description, internal offense class code & code description, jurisdiction code & code description. As the code itself would not tell much information about the features, we removed these codes and only kept the corresponding short description the code refers to.

```{r}
#package loading
library(dplyr)
library(tidyr)
library(tidyverse)
library(redav)
```


```{r}
# df_clean <- df %>% select(-c(JURIS_DESC, JURISDICTION_CODE, PARKS_NM, HADEVELOPT, HOUSING_PSA, TRANSIT_DISTRICT, STATION_NAME, ADDR_PCT_CD, KY_CD, PD_CD))
#head(df,5)
```



## Missing value analysis

We performed analysis on missing values to see if there are any patterns of the missing data. 

We first wanted to know the amount of missing data in each feature and which feature(s) contain insufficient data to perform the analysis with. From the distribution of missing data of features with missing values, we can see that "TRANSIT_DISTRICT" (transit district in which the offense occurred) and "HOUSING_PSA" (development level code) have about 97% and 30% of missing rows separately, which are the top two features with missing values among 35 features. There are also other features with rows missing values, such as "PD_CD" (internal classification code) and "JURISDICTION_CODE", but the percentages are below 1%, which are not explicitly displayed in the plot.

```{r}
data=read.csv("~/NYPD_Complaint_Data_Historic.csv")
df<-as.data.frame(data)
col_missing <- colSums(is.na(df)) %>% sort(decreasing = TRUE)
col_missing = (col_missing/nrow(df))*100
```


```{r}
tidy_missing = 
    as.data.frame(col_missing) %>% 
    rownames_to_column("id") %>% 
    gather(key, value, -id) 

ggplot(tidy_missing[tidy_missing$value>0,], aes(x=fct_rev(fct_reorder(id,value,.desc = FALSE)),y=value)) +
geom_bar(stat='identity',fill = "cornflowerblue") +
#scale_x_discrete(breaks = levels(tidy_missing$id)[c(T,T, rep(F, 33))]) +
scale_x_discrete(guide = guide_axis(n.dodge=3))+
xlab("") + 
ylab("Percentage of Missing Rows")
```

In addition to the percentage of missing values in each feature, we can also show the missing patterns. By choosing four variables with largest number of missing values, there are 6 missing patterns in the dataset, where about 65% of rows belong to the first pattern and over 25% of rows in the second pattern. Therefore, there are lots of rows missing data in several features, as it's impossible to fill these values as they are categorial features with text data, so we decided to remove them in the analysis.


```{r}
plot_missing(df[,c(tidy_missing[tidy_missing$value>0,]$id)][,1:4], percent = TRUE)
```


